{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = http://i1377.photobucket.com/albums/ah57/drmingle1/Jupyter%20-%20Healthcare%20Data%20Science/BosDataFestival2015_zpsqpbcvzw8.png alt=\"Matrix\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "# Finding Type II Diabetes In A Patient Population\n",
    "Given a year of EHR data for patients without Diabetes, we predict which patients will be diagnosed with Diabetes in the next year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************************\n",
    "#Excercise 4\n",
    "\n",
    "Choose between A and B.\n",
    "***************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Scientist A**\n",
    "\n",
    "#PHASE IV: Modeling\n",
    "\n",
    "######Select Modeling Technique\n",
    "\n",
    "1. Modeling technique\n",
    "We have used a combination of Gradient Boosting Machines (GBM) and Random Forest (RF). Each are information-based learners. As with all tree representations, a decision tree consists of a root node, interior nodes, and leaf nodes that are connected by branches. Each non-leaf node (root and interior) in the tree specifies a test to be carried out on a descriptive feature. The number of possible levels that a descriptive feacture can take determines the number of downward branches from a non-leaf node. Each of ht eleaf nodes specifies a predicted level of the target feature. \n",
    "\n",
    "The process of using a decision tree to make a prediction for a query instance starts by testing the value of the descriptive feature at the root node of the tree. The result of this test dtermines which of hte root node's children  the process should then descend to. These two steps of testing the value of a descriptive feature and descending a level in the tree are then repeated until the process comes to a leaf node at which a prediction can be made. \n",
    "\n",
    "Regarding Random Forest, this algorithm is an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests correct for decision trees' habit of overfitting to their training set.The algorithm for inducing a random forest was developed in order to construct a collection of decision trees with controlled variance.\n",
    "\n",
    "2. Modeling assumptions\n",
    "\n",
    "GBM aims to work against fitting the training set too closely and employs regularlization to enhance the generalization of the model. One natural regularization parameter is the number of gradient boosting iternations. Increasing the number of trees reduces the error on the training set, but setting it too high may lead to overfitting. \n",
    "\n",
    "Regularization by shrinkage, empiracally it has been found that using smal learning rates yields dramatic improvements in model's generalization ability over gradient boosting without shrinkage. However, it comes with higher computational costs. In addition, lower learning rates require more iterations. \n",
    "\n",
    "Subsampling is used to define an out-of-bag estimate of the prediction performance improvement by evaluating predictions on those observations which were not eused in the building of the next base learner. Out-of-bag estimates help avoid the need for an independent validation dataset, but often undersestimate actual peroformance improvement and the optimal number of iterations. \n",
    "\n",
    "Another useful regularization technique for gradient boosted trees is to penalize model complexisty of the learned model. The model complexity can be defined proportional number of leaves in the learned trees. \n",
    "\n",
    "Concerning RF, Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called \"feature bagging\". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the trees, causing them to become correlated.\n",
    "\n",
    "######Generate Test Design\n",
    "\n",
    "Regarding test design, we made use of a hold-out test set. This was created by randomly sampling a portion of the data in the ABT we created during the Data Preparation phase. It is important to not that this random sample is never used in the training process but reserved until all the model has been trained, when we would like to evaluate its performance. The performanc eof th emodel on the test set is a better measure of how hte model is liekly to peform when actually deployed and shows how well the model can generalize beyond th einstances used to train it. We felt that the most important rule in evaluating models was not to use the same data sample both to evaluate the peformance of a predictiv emodel and to train it. \n",
    "\n",
    "\n",
    "######Build Model\n",
    "Below are the parameter settings and results for the various models:\n",
    "\n",
    "***Graident Boosting Machine***\n",
    "\n",
    "GBM models were fitted with differing parameters. For cross-validation stopping the wrapper 'gbm.step' in dismo R package was used, only modified for add n.minobsinnode parameter.\n",
    "\n",
    "GBM Model | CV Error | Folds | Trees | Depth | Shrinkage | Bag Frac | Node Size | Tolerance\n",
    "-|-|-|-|-|-|-|-\n",
    "    gbm 10_5 0.003_0.80_30 | 0.31210 | 10|7,550| 5 | 0.003|0.80|30|0.001\n",
    "    gbm10_5_0.003_0.80_30_tolhalf_ext |\t0.31153|\t10|\t8,500|\t5|\t0.003|\t0.80|\t30|\t0.0005\n",
    "    gbm10_5_0.0025_0.80_30\t|0.31319\t|10\t|8,000|\t5|\t0.003|\t0.80|\t30|\t0.001\n",
    "    gbm10_5_0.0025_0.80_30_ext|\t0.31312\t|10|\t7,750|\t5|\t0.0025|\t0.80|\t30|\t0.001\n",
    "    gbm10_5_0.0025_0.80_30_tolhalf\t|0.31122\t|10\t|11,000|\t5|\t0.0025|\t0.80|\t30|\t0.0005\n",
    "    gbm10_5_0.0025_0.80_30_tolhalf_ext\t|0.31139|\t10\t|10,450|\t5|\t0.0025|\t0.80|\t30|\t0.0005\n",
    "    gbm20_5_0.002_0.80_10\t|0.31049\t|20|\t13,450|\t5\t|0.002|\t0.80|\t10|\t0.001\n",
    "    gbm20_5_0.002_0.80_15\t|0.31040\t|20\t|12,950\t|5\t|0.002\t|0.80\t|15\t|0.001\n",
    "    gbm20_5_0.0025_0.80_20\t|0.30878\t|20\t|12,300\t|5\t|0.0025\t|0.80\t|20\t|0.001\n",
    "    gbm20_5_0.0025_0.80_40\t|0.31040\t|20\t|10,500\t|5\t|0.0025\t|0.80\t|40\t|0.001\n",
    "    gbm20_6_0.002_0.80_30\t|0.30931\t|20\t|12,200\t|6\t|0.002\t|0.80\t|30\t|0.001\n",
    "\n",
    "    \n",
    "***Random Forest***\n",
    "Four models were fitted with Random Forest, all parameters by defect except the detailed in table.\n",
    "\n",
    "RF Model | OOB MSE | Trees | Node Size\n",
    "-|-|-|-\n",
    "    RF1 | 0.10055| 15,000 | 5\n",
    "    RF5\t|0.10076|\t30,000|\t15\n",
    "    RF2\t|0.10089|\t15,000|\t20\n",
    "    RF3\t|0.10102|\t15,000|\t40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**===================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the actual modeling code produced: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Model F\n",
    "set.seed(223646)\n",
    "gbm10_5_0.003_0.80_30<-gbm.step(Patient2,gbm.x=c(3:ncol(Patient2)),gbm.y=2,\n",
    "tree.complexity=5,\n",
    "n.folds=10,n.trees=50, learning.rate=0.003,bag.fraction=0.80,n.minobsinnode=30,tolerance = 0.001,\n",
    "family=\"bernoulli\",max.trees=30000,keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit=TRUE)\n",
    "save(gbm10_5_0.003_0.80_30,file=\"c://dmii/models/gbm10_5_0.003_0.80_30.RData\",compress=TRUE)\n",
    "pred<-predict.gbm(gbm10_5_0.003_0.80_30,Patient[Patient$dmIndicator==-1,dmii],n.trees=gbm10_5_0.003_0.80_30$gbm.call$best.trees,type=\"response\")\n",
    "sentdata<- cbind(as.character(Patient$PatientGuid[Patient$dmIndicator==-1]),as.numeric(pred)) \n",
    "colnames(sentdata)<-c('PatientGuid','dmIndicator')\n",
    "write.csv(sentdata, file=\"c://dmii/models/gbm10_5_0.003_0.80_30.csv\", row.names = FALSE) \n",
    "save(gbm10_5_0.003_0.80_30,file=\"c://dmii/models/gbm10_5_0.003_0.80_30.RData\",compress=TRUE)\n",
    "## cv fold predictions set for stacking models\n",
    "cvestim<-cbind(as.character(Patient2$PatientGuid),exp(gbm10_5_0.003_0.80_30$fold.fit)/(1+exp(gbm10_5_0.003_0.80_30$fold.fit)))\n",
    "colnames(cvestim)<-c('PatientGuid','gbm10_5_0.003_0.80_30')\n",
    "write.csv(cvestim,file=\"c://dmii/models/gbm10_5_0.003_0.80_30_cvest.csv\", row.names = FALSE)\n",
    "\n",
    "#  Model 5\n",
    "set.seed(223646)\n",
    "gbm20_5_0.002_0.80_10<-gbm.step(Patient2,gbm.x=c(3:ncol(Patient2)),gbm.y=2,\n",
    "tree.complexity=5,\n",
    "n.folds=20,n.trees=50, learning.rate=0.002,bag.fraction=0.80,n.minobsinnode=15,tolerance = 0.0005,\n",
    "family=\"bernoulli\",max.trees=30000,keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit=TRUE)\n",
    "save(gbm20_5_0.002_0.80_10,file=\"c://dmii/models/gbm20_5_0.002_0.80_10.RData\",compress=TRUE)\n",
    "pred<-predict.gbm(gbm20_5_0.002_0.80_10,Patient[Patient$dmIndicator==-1,dmii],n.trees=gbm20_5_0.002_0.80_10$gbm.call$best.trees,type=\"response\")\n",
    "sentdata<- cbind(as.character(Patient$PatientGuid[Patient$dmIndicator==-1]),as.numeric(pred)) \n",
    "colnames(sentdata)<-c('PatientGuid','dmIndicator')\n",
    "write.csv(sentdata, file=\"c://dmii/models/gbm20_5_0.002_0.80_10.csv\", row.names = FALSE) \n",
    "## cv fold predictions set for stacking models\n",
    "cvestim<-cbind(as.character(Patient2$PatientGuid),exp(gbm20_5_0.002_0.80_10$fold.fit)/(1+exp(gbm20_5_0.002_0.80_10$fold.fit)))\n",
    "colnames(cvestim)<-c('PatientGuid','gbm20_5_0.002_0.80_10')\n",
    "write.csv(cvestim,file=\"c://dmii/models/gbm20_5_0.002_0.80_10_cvest.csv\", row.names = FALSE)\n",
    "\n",
    "#  Model 4\n",
    "set.seed(223646)\n",
    "gbm20_5_0.002_0.80_15<-gbm.step(Patient2,gbm.x=c(3:ncol(Patient2)),gbm.y=2,\n",
    "tree.complexity=5,\n",
    "n.folds=20,n.trees=50, learning.rate=0.002,bag.fraction=0.80,n.minobsinnode=15,tolerance = 0.0005,\n",
    "family=\"bernoulli\",max.trees=30000,keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit=TRUE)\n",
    "save(gbm20_5_0.002_0.80_15,file=\"c://dmii/models/gbm20_5_0.002_0.80_15.RData\",compress=TRUE)\n",
    "pred<-predict.gbm(gbm20_5_0.002_0.80_15,Patient[Patient$dmIndicator==-1,dmii],n.trees=gbm20_5_0.002_0.80_15$gbm.call$best.trees,type=\"response\")\n",
    "sentdata<- cbind(as.character(Patient$PatientGuid[Patient$dmIndicator==-1]),as.numeric(pred)) \n",
    "colnames(sentdata)<-c('PatientGuid','dmIndicator')\n",
    "write.csv(sentdata, file=\"c://dmii/models/gbm20_5_0.002_0.80_15.csv\", row.names = FALSE) \n",
    "## cv fold predictions set for stacking models\n",
    "cvestim<-cbind(as.character(Patient2$PatientGuid),exp(gbm20_5_0.002_0.80_15$fold.fit)/(1+exp(gbm20_5_0.002_0.80_15$fold.fit)))\n",
    "colnames(cvestim)<-c('PatientGuid','gbm20_5_0.002_0.80_15')\n",
    "write.csv(cvestim,file=\"c://dmii/models/gbm20_5_0.002_0.80_15_cvest.csv\", row.names = FALSE)\n",
    "\n",
    "#  Model 3\n",
    "set.seed(223646)\n",
    "gbm20_6_0.002_0.80_30<-gbm.step(Patient2,gbm.x=c(3:ncol(Patient2)),gbm.y=2,\n",
    "tree.complexity=6,\n",
    "n.folds=20,n.trees=50, learning.rate=0.002,bag.fraction=0.80,n.minobsinnode=30,tolerance = 0.0005,\n",
    "family=\"bernoulli\",max.trees=30000,keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit=TRUE)\n",
    "save(gbm20_6_0.002_0.80_30,file=\"c://dmii/models/gbm20_6_0.002_0.80_30.RData\",compress=TRUE)\n",
    "pred<-predict.gbm(gbm20_6_0.002_0.80_30,Patient[Patient$dmIndicator==-1,dmii],n.trees=gbm20_6_0.002_0.80_30$gbm.call$best.trees,type=\"response\")\n",
    "sentdata<- cbind(as.character(Patient$PatientGuid[Patient$dmIndicator==-1]),as.numeric(pred)) \n",
    "colnames(sentdata)<-c('PatientGuid','dmIndicator')\n",
    "write.csv(sentdata, file=\"c://dmii/models/gbm20_6_0.002_0.80_30.csv\", row.names = FALSE) \n",
    "## cv fold predictions set for stacking models\n",
    "cvestim<-cbind(as.character(Patient2$PatientGuid),exp(gbm20_6_0.002_0.80_30$fold.fit)/(1+exp(gbm20_6_0.002_0.80_30$fold.fit)))\n",
    "colnames(cvestim)<-c('PatientGuid','gbm20_6_0.002_0.80_30')\n",
    "write.csv(cvestim,file=\"c://dmii/models/gbm20_6_0.002_0.80_30_cvest.csv\", row.names = FALSE)\n",
    "\n",
    "#  Model 2\n",
    "set.seed(223646)\n",
    "gbm20_5_0.0025_0.80_40<-gbm.step(Patient2,gbm.x=c(3:ncol(Patient2)),gbm.y=2,\n",
    "tree.complexity=5,\n",
    "n.folds=20,n.trees=50, learning.rate=0.0025,bag.fraction=0.80,n.minobsinnode=40,tolerance = 0.0005,\n",
    "family=\"bernoulli\",max.trees=30000,keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit=TRUE)\n",
    "save(gbm20_5_0.0025_0.80_40,file=\"c://dmii/models/gbm20_5_0.0025_0.80_40.RData\",compress=TRUE)\n",
    "pred<-predict.gbm(gbm20_5_0.0025_0.80_40,Patient[Patient$dmIndicator==-1,dmii],n.trees=gbm20_5_0.0025_0.80_40$gbm.call$best.trees,type=\"response\")\n",
    "sentdata<- cbind(as.character(Patient$PatientGuid[Patient$dmIndicator==-1]),as.numeric(pred)) \n",
    "colnames(sentdata)<-c('PatientGuid','dmIndicator')\n",
    "write.csv(sentdata, file=\"c://dmii/models/gbm20_5_0.0025_0.80_40.csv\", row.names = FALSE) \n",
    "## cv fold predictions set for stacking models\n",
    "cvestim<-cbind(as.character(Patient2$PatientGuid),exp(gbm20_5_0.0025_0.80_40$fold.fit)/(1+exp(gbm20_5_0.0025_0.80_40$fold.fit)))\n",
    "colnames(cvestim)<-c('PatientGuid','gbm20_5_0.0025_0.80_40')\n",
    "write.csv(cvestim,file=\"c://dmii/models/gbm20_5_0.0025_0.80_40_cvest.csv\", row.names = FALSE)\n",
    "\n",
    "#  Model 1\n",
    "set.seed(223646)\n",
    "gbm20_5_0.0025_0.80_20<-gbm.step(Patient2,gbm.x=c(3:ncol(Patient2)),gbm.y=2,\n",
    "tree.complexity=5,\n",
    "n.folds=20,n.trees=50, learning.rate=0.0025,bag.fraction=0.80,n.minobsinnode=20,tolerance = 0.0005,\n",
    "family=\"bernoulli\",max.trees=30000,keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit=TRUE)\n",
    "save(gbm20_5_0.0025_0.80_20,file=\"c://dmii/models/gbm20_5_0.0025_0.80_20.RData\",compress=TRUE)\n",
    "pred<-predict.gbm(gbm20_5_0.0025_0.80_20,Patient[Patient$dmIndicator==-1,dmii],n.trees=gbm20_5_0.0025_0.80_20$gbm.call$best.trees,type=\"response\")\n",
    "sentdata<- cbind(as.character(Patient$PatientGuid[Patient$dmIndicator==-1]),as.numeric(pred)) \n",
    "colnames(sentdata)<-c('PatientGuid','dmIndicator')\n",
    "write.csv(sentdata, file=\"c://dmii/models/gbm20_5_0.0025_0.80_20.csv\", row.names = FALSE) \n",
    "## cv fold predictions set for stacking models\n",
    "cvestim<-cbind(as.character(Patient2$PatientGuid),exp(gbm20_5_0.0025_0.80_20$fold.fit)/(1+exp(gbm20_5_0.0025_0.80_20$fold.fit)))\n",
    "colnames(cvestim)<-c('PatientGuid','gbm20_5_0.0025_0.80_20')\n",
    "write.csv(cvestim,file=\"c://dmii/models/gbm20_5_0.0025_0.80_20_cvest.csv\", row.names = FALSE)\n",
    " \n",
    "#Model H\n",
    "set.seed(223646)\n",
    "gbm10_5_0.0025_0.80_30_tolhalf<-gbm.step(Patient2,gbm.x=c(3:ncol(Patient2)),gbm.y=2,\n",
    "tree.complexity=5,\n",
    "n.folds=10,n.trees=50, learning.rate=0.0025,bag.fraction=0.80,n.minobsinnode=30,tolerance = 0.0005,\n",
    "family=\"bernoulli\",max.trees=30000,keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit=TRUE)\n",
    "save(gbm10_5_0.0025_0.80_30_tolhalf,file=\"c://dmii/models/gbm10_5_0.0025_0.80_30_tolhalf.RData\",compress=TRUE)\n",
    "pred<-predict.gbm(gbm10_5_0.0025_0.80_30_tolhalf,Patient[Patient$dmIndicator==-1,dmii],n.trees=gbm10_5_0.0025_0.80_30_tolhalf$gbm.call$best.trees,type=\"response\")\n",
    "sentdata<- cbind(as.character(Patient$PatientGuid[Patient$dmIndicator==-1]),as.numeric(pred)) \n",
    "colnames(sentdata)<-c('PatientGuid','dmIndicator')\n",
    "write.csv(sentdata, file=\"c://dmii/models/gbm10_5_0.0025_0.80_30_tolhalf.csv\", row.names = FALSE) \n",
    "save(gbm10_5_0.0025_0.80_30_tolhalf,file=\"c://dmii/models/gbm10_5_0.0025_0.80_30_tolhalf.RData\",compress=TRUE)\n",
    "## cv fold predictions set for stacking models\n",
    "cvestim<-cbind(as.character(Patient2$PatientGuid),exp(gbm10_5_0.0025_0.80_30_tolhalf$fold.fit)/(1+exp(gbm10_5_0.0025_0.80_30_tolhalf$fold.fit)))\n",
    "colnames(cvestim)<-c('PatientGuid','gbm10_5_0.0025_0.80_30_tolhalf')\n",
    "write.csv(cvestim,file=\"c://dmii/models/gbm10_5_0.0025_0.80_30_tolhalf_cvest.csv\", row.names = FALSE)\n",
    "\n",
    "#Model E\n",
    "set.seed(223646)\n",
    "gbm10_5_0.0025_0.80_30<-gbm.step(Patient2,gbm.x=c(3:ncol(Patient2)),gbm.y=2,\n",
    "tree.complexity=5,\n",
    "n.folds=10,n.trees=50, learning.rate=0.0025,bag.fraction=0.80,n.minobsinnode=30,tolerance = 0.001,\n",
    "family=\"bernoulli\",max.trees=30000,keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit=TRUE)\n",
    "save(gbm10_5_0.0025_0.80_30,file=\"c://dmii/models/gbm10_5_0.0025_0.80_30.RData\",compress=TRUE)\n",
    "pred<-predict.gbm(gbm10_5_0.0025_0.80_30,Patient[Patient$dmIndicator==-1,dmii],n.trees=gbm10_5_0.0025_0.80_30$gbm.call$best.trees,type=\"response\")\n",
    "sentdata<- cbind(as.character(Patient$PatientGuid[Patient$dmIndicator==-1]),as.numeric(pred)) \n",
    "colnames(sentdata)<-c('PatientGuid','dmIndicator')\n",
    "write.csv(sentdata, file=\"c://dmii/models/gbm10_5_0.0025_0.80_30.csv\", row.names = FALSE) \n",
    "save(gbm10_5_0.0025_0.80_30,file=\"c://dmii/models/gbm10_5_0.0025_0.80_30.RData\",compress=TRUE)\n",
    "## cv fold predictions set for stacking models\n",
    "cvestim<-cbind(as.character(Patient2$PatientGuid),exp(gbm10_5_0.0025_0.80_30$fold.fit)/(1+exp(gbm10_5_0.0025_0.80_30$fold.fit)))\n",
    "colnames(cvestim)<-c('PatientGuid','gbm10_5_0.0025_0.80_30')\n",
    "write.csv(cvestim,file=\"c://dmii/models/gbm10_5_0.0025_0.80_30_cvest.csv\", row.names = FALSE)\n",
    "\n",
    "### Models with a few more features (really not necesaries)\n",
    "\n",
    "Patient2<-Patient[Patient$dmIndicator!=-1,c(dmii,exten)]\n",
    "\n",
    "#Model A\n",
    "set.seed(223646)\n",
    "gbm10_5_0.0025_0.80_30_tolhalf_ext<-gbm.step(Patient2,gbm.x=c(3:ncol(Patient2)),gbm.y=2,\n",
    "tree.complexity=5,\n",
    "n.folds=10,n.trees=50, learning.rate=0.0025,bag.fraction=0.80,n.minobsinnode=30,tolerance = 0.0005,\n",
    "family=\"bernoulli\",max.trees=30000,keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit=TRUE)\n",
    "save(gbm10_5_0.0025_0.80_30_tolhalf_ext,file=\"c://dmii/models/gbm10_5_0.0025_0.80_30_tolhalf_ext.RData\",compress=TRUE)\n",
    "pred<-predict.gbm(gbm10_5_0.0025_0.80_30_tolhalf_ext,Patient[Patient$dmIndicator==-1,c(dmii,exten)],n.trees=gbm10_5_0.0025_0.80_30_tolhalf_ext$gbm.call$best.trees,type=\"response\")\n",
    "sentdata<- cbind(as.character(Patient$PatientGuid[Patient$dmIndicator==-1]),as.numeric(pred)) \n",
    "colnames(sentdata)<-c('PatientGuid','dmIndicator')\n",
    "write.csv(sentdata, file=\"c://dmii/models/gbm10_5_0.0025_0.80_30_tolhalf_ext.csv\", row.names = FALSE) \n",
    "## cv fold predictions set for stacking models\n",
    "cvestim<-cbind(as.character(Patient2$PatientGuid),exp(gbm10_5_0.0025_0.80_30_tolhalf_ext$fold.fit)/(1+exp(gbm10_5_0.0025_0.80_30_tolhalf_ext$fold.fit)))\n",
    "colnames(cvestim)<-c('PatientGuid','gbm10_5_0.0025_0.80_30_tolhalf_ext')\n",
    "write.csv(cvestim,file=\"c://dmii/models/gbm10_5_0.0025_0.80_30_tolhalf_ext_cvest.csv\", row.names = FALSE)\n",
    "\n",
    "#Model D\n",
    "set.seed(223646)\n",
    "gbm10_5_0.0025_0.80_30_ext<-gbm.step(Patient2,gbm.x=c(3:ncol(Patient2)),gbm.y=2,\n",
    "tree.complexity=5,\n",
    "n.folds=10,n.trees=50, learning.rate=0.0025,bag.fraction=0.80,n.minobsinnode=30,tolerance = 0.001,\n",
    "family=\"bernoulli\",max.trees=30000,keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit=TRUE)\n",
    "save(gbm10_5_0.0025_0.80_30_ext,file=\"c://dmii/models/gbm10_5_0.0025_0.80_30_ext.RData\",compress=TRUE)\n",
    "pred<-predict.gbm(gbm10_5_0.0025_0.80_30_ext,Patient[Patient$dmIndicator==-1,c(dmii,exten)],n.trees=gbm10_5_0.0025_0.80_30_ext$gbm.call$best.trees,type=\"response\")\n",
    "sentdata<- cbind(as.character(Patient$PatientGuid[Patient$dmIndicator==-1]),as.numeric(pred)) \n",
    "colnames(sentdata)<-c('PatientGuid','dmIndicator')\n",
    "write.csv(sentdata, file=\"c://dmii/models/gbm10_5_0.0025_0.80_30_ext.csv\", row.names = FALSE) \n",
    "## cv fold predictions set for stacking models\n",
    "cvestim<-cbind(as.character(Patient2$PatientGuid),exp(gbm10_5_0.0025_0.80_30_ext$fold.fit)/(1+exp(gbm10_5_0.0025_0.80_30_ext$fold.fit)))\n",
    "colnames(cvestim)<-c('PatientGuid','gbm10_5_0.0025_0.80_30_ext')\n",
    "write.csv(cvestim,file=\"c://dmii/models/gbm10_5_0.0025_0.80_30_ext_cvest.csv\", row.names = FALSE)\n",
    " \n",
    "#Model B\n",
    "set.seed(223646)\n",
    "gbm10_5_0.003_0.80_30_tolhalf_ext<-gbm.step(Patient2,gbm.x=c(3:ncol(Patient2)),gbm.y=2,\n",
    "tree.complexity=5,\n",
    "n.folds=10,n.trees=50, learning.rate=0.003,bag.fraction=0.80,n.minobsinnode=30,tolerance = 0.0005,\n",
    "family=\"bernoulli\",max.trees=30000,keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit=TRUE)\n",
    "save(gbm10_5_0.003_0.80_30_tolhalf_ext,file=\"c://dmii/models/gbm10_5_0.003_0.80_30_tolhalf_ext.RData\",compress=TRUE)\n",
    "pred<-predict.gbm(gbm10_5_0.003_0.80_30_tolhalf_ext,Patient[Patient$dmIndicator==-1,c(dmii,exten)],n.trees=gbm10_5_0.003_0.80_30_tolhalf_ext$gbm.call$best.trees,type=\"response\")\n",
    "sentdata<- cbind(as.character(Patient$PatientGuid[Patient$dmIndicator==-1]),as.numeric(pred)) \n",
    "colnames(sentdata)<-c('PatientGuid','dmIndicator')\n",
    "write.csv(sentdata, file=\"c://dmii/models/gbm10_5_0.003_0.80_30_tolhalf_ext.csv\", row.names = FALSE) \n",
    "## cv fold predictions set for stacking models\n",
    "cvestim<-cbind(as.character(Patient2$PatientGuid),exp(gbm10_5_0.003_0.80_30_tolhalf_ext$fold.fit)/(1+exp(gbm10_5_0.003_0.80_30_tolhalf_ext$fold.fit)))\n",
    "colnames(cvestim)<-c('PatientGuid','gbm10_5_0.003_0.80_30_tolhalf_ext')\n",
    "write.csv(cvestim,file=\"c://dmii/models/gbm10_5_0.003_0.80_30_tolhalf_ext_cvest.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Scientist B**\n",
    "\n",
    "#PHASE IV: Modeling\n",
    "\n",
    "######Select Modeling Technique\n",
    "This section refers to the specific modeling technique.\n",
    "\n",
    "1. Modeling technique\n",
    "    * Document the actual modeling technique that is to be used:\n",
    "2. Modeling assumptions\n",
    "    * Record the assumption that the model makes about the data:\n",
    "\n",
    "######Generate Test Design\n",
    "Describe how we plan to test the model's quality and validity of the model.\n",
    "\n",
    "1. Test design\n",
    "    * Describe the plan for training:\n",
    "    * Describe the plan for testing:\n",
    "    * Describe the plan for evaluating models:\n",
    "\n",
    "######Build Model\n",
    "Run the modeling tool on the prepared dataset to create one or more models.\n",
    "\n",
    "1. Parameter settings\n",
    "    * List the paramaters and their chosen values, along with the choice of parameters settings:\n",
    "2. Models\n",
    "    * These are the actual models produced by the modeling tool, not a report.\n",
    "3. Model description\n",
    "    * Describe the resulting models. Report on the interpretation of the models and document any difficultites encountered with their meanings.\n",
    "\n",
    "######Assess Model\n",
    "The Data Scientist ranks models according to the evaluation criteria. As much as possible, business objective and business success criteria should be taken into account.\n",
    "\n",
    "1. Model assessment\n",
    "    * Summarize results of this task, list quality of generated models (in terms of accuracy and quality):\n",
    "    \n",
    "2. Revised paramter settings\n",
    "    * Revised parameter settings and tune them for the next run in the build model task:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
